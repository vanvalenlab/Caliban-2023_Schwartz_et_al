{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "Resave data as a set of tiff files in order to match Cell Tracking Challenge conventions which are expected by EmbedTrack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tifffile import imwrite\n",
    "\n",
    "from deepcell_tracking.isbi_utils import trk_to_isbi\n",
    "from deepcell_tracking.trk_io import load_trks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = '/data/test.trks'\n",
    "data_dir = '/EmbedTrack/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7da521-7db3-4c25-8ecf-f4e45ceec799",
   "metadata": {},
   "source": [
    "Load the test split of the tracking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd83165-efba-4cba-8961-6b0e9a3329df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_trks(source_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ab7ad-3309-4655-b1fc-fd270b52fb26",
   "metadata": {},
   "source": [
    "Convert each batch of the test split to the standard ISBI format which is compatible with most of the models that we will tst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d71c85-250b-49b9-8747-5619158a8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_no in range(len(data['lineages'])):\n",
    "    # Build subdirectories for data\n",
    "    raw_dir = os.path.join(data_dir, '{:03}'.format(batch_no + 1))\n",
    "    gt_dir = os.path.join(data_dir, '{:03}_GT'.format(batch_no + 1))\n",
    "    seg_dir = os.path.join(gt_dir, 'SEG')\n",
    "    tra_dir = os.path.join(gt_dir, 'TRA')\n",
    "    \n",
    "    # Create directories if needed\n",
    "    for d in (raw_dir, gt_dir, seg_dir, tra_dir):\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "                \n",
    "    # Pull out relevant data for this batch\n",
    "    x = data['X'][batch_no]\n",
    "    y = data['y'][batch_no]\n",
    "    lineages = data['lineages'][batch_no]\n",
    "    \n",
    "    # Need to translate lineages and adjust images to match restrictive ISBI format\n",
    "    # Prepare output txt\n",
    "    text_file = os.path.join(tra_dir, 'man_track.txt')\n",
    "    df = trk_to_isbi(lineages)\n",
    "    df.to_csv(text_file, sep=' ')\n",
    "    \n",
    "    # Determine which frames are zero padding\n",
    "    frames = np.sum(y, axis=(1,2)) # True if image not blank\n",
    "    good_frames = np.where(frames)[0]\n",
    "    # We assume here that the empty frames are at the end of the movie (padding rather than skipped)\n",
    "    movie_len = len(good_frames)\n",
    "    \n",
    "    # Save each frame of the movie as an individual tif\n",
    "    channel = 0 # These images should only have one channel\n",
    "    for i in range(movie_len):\n",
    "        name_raw = os.path.join(raw_dir, 't{:03}_.tif'.format(i))\n",
    "        name_tracked_seg = os.path.join(seg_dir, 'man_seg{:03}.tif'.format(i))\n",
    "        name_tracked_tra = os.path.join(tra_dir, 'man_track{:03}.tif'.format(i))\n",
    "        \n",
    "        imwrite(name_raw, X[batch_no, i, ..., channel])\n",
    "        imwrite(name_tracked_seg, y[batch_no, i, ..., channel].astype('uint16'))\n",
    "        imwrite(name_tracked_tra, y[batch_no, i, ..., channel].astype('uint16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EmbedTrack Inference\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> This notebook must be moved into the `embedtrack` folder in order to correctly import `embedtrack` modules.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4dd6c-c1a2-4edd-a36e-f97aaf0a9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ac667-535c-430c-bd16-4fa6c69d3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_dir = '/EmbedTrack/KIT-Loe-GE/models/Fluo-N2DL-HeLa'\n",
    "model_path = os.path.join(model_dir, m, 'best_iou_model.pth')\n",
    "config_file = os.path.join(model_dir, m, 'config.json')\n",
    "\n",
    "data_dir = '/EmbedTrack/data'\n",
    "\n",
    "pattern = re.compile('\\d{3}')\n",
    "data_ids = [f for f in os.listdir(data_dir) if pattern.fullmatch(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c934f-4ee4-43f7-a5d4-742e97696f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv_embedtrack/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal.windows import gaussian\n",
    "import tifffile\n",
    "import torch\n",
    "\n",
    "from embedtrack.infer.inference import (\n",
    "    extend_grid,\n",
    "    infer_sequence,\n",
    "    create_inference_dict,\n",
    "    calc_padded_img_size,\n",
    "    init_model,\n",
    "    foi_correction,\n",
    "    rename_to_ctc_format,\n",
    "    device,\n",
    ")\n",
    "from embedtrack.infer.infer_ctc_data import fill_empty_frames\n",
    "from embedtrack.utils.clustering import Cluster\n",
    "from embedtrack.utils.create_dicts import create_model_dict\n",
    "from embedtrack.utils.utils import get_img_files\n",
    "\n",
    "\n",
    "# This is a modified versioon of embedtrack.infer.infer_ctc_data.inference\n",
    "# which eliminates the requirement that the data name matches the model name\n",
    "\n",
    "def inference(raw_data_path, model_path, config_file, batch_size=32):\n",
    "    \"\"\"\n",
    "    Segment and track a ctc dataset using a trained EmbedTrack model.\n",
    "    Args:\n",
    "        raw_data_path: string\n",
    "            Path to the raw images\n",
    "        model_path: string\n",
    "            Path to the weights of the trained model\n",
    "        config_file: string\n",
    "            Path to the configuration of the model\n",
    "        batch_size: int\n",
    "            batch size during inference\n",
    "    \"\"\"\n",
    "    raw_data_path = Path(raw_data_path)\n",
    "    model_path = Path(model_path)\n",
    "\n",
    "    data_id = raw_data_path.parts[-1]\n",
    "    data_set = raw_data_path.parts[-2]\n",
    "\n",
    "    ctc_res_path = raw_data_path.parent / (data_id + \"_RES\")\n",
    "    temp_res_path = \"./temp\"\n",
    "    if not os.path.exists(temp_res_path):\n",
    "        os.makedirs(temp_res_path)\n",
    "    else:\n",
    "        shutil.rmtree(temp_res_path)\n",
    "\n",
    "    # These lines are the modification\n",
    "    # if data_set not in model_path.as_posix():\n",
    "    #     raise Warning(f\"The model {model_path} is not named as the data set {data_set}\")\n",
    "\n",
    "    overlap = 0.25\n",
    "\n",
    "    with open(config_file) as file:\n",
    "        train_config = json.load(file)\n",
    "\n",
    "    model_class = train_config[\"model_dict\"][\"name\"]\n",
    "    crop_size = train_config[\"train_dict\"][\"crop_size\"]\n",
    "\n",
    "    image_size = tifffile.imread(\n",
    "        os.path.join(raw_data_path, os.listdir(raw_data_path)[0])\n",
    "    ).shape\n",
    "\n",
    "    project_config = dict(\n",
    "        image_dir=raw_data_path,\n",
    "        res_dir=temp_res_path,\n",
    "        model_cktp_path=model_path,\n",
    "        model_class=model_class,\n",
    "        grid_y=train_config[\"grid_dict\"][\"grid_y\"],\n",
    "        grid_x=train_config[\"grid_dict\"][\"grid_x\"],\n",
    "        pixel_y=train_config[\"grid_dict\"][\"pixel_y\"],\n",
    "        pixel_x=train_config[\"grid_dict\"][\"pixel_x\"],\n",
    "        overlap=overlap,\n",
    "        crop_size=crop_size,  # multiple of 2\n",
    "        img_size=image_size,\n",
    "        padded_img_size=None,\n",
    "    )\n",
    "    project_config[\"padded_img_size\"] = calc_padded_img_size(\n",
    "        project_config[\"img_size\"],\n",
    "        project_config[\"crop_size\"],\n",
    "        project_config[\"overlap\"],\n",
    "    )[0]\n",
    "    window_function_1d = gaussian(\n",
    "        project_config[\"crop_size\"], project_config[\"crop_size\"] // 4\n",
    "    )\n",
    "    project_config[\"window_func\"] = window_function_1d.reshape(\n",
    "        -1, 1\n",
    "    ) * window_function_1d.reshape(1, -1)\n",
    "\n",
    "    dataset_dict = create_inference_dict(\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # init model\n",
    "    input_channels = train_config[\"model_dict\"][\"kwargs\"][\"input_channels\"]\n",
    "    n_classes = train_config[\"model_dict\"][\"kwargs\"][\"n_classes\"]\n",
    "    model_dict = create_model_dict(\n",
    "        input_channels=input_channels,\n",
    "        n_classes=n_classes,\n",
    "    )\n",
    "    model = init_model(model_dict, project_config)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # clustering\n",
    "    cluster = Cluster(\n",
    "        project_config[\"grid_y\"],\n",
    "        project_config[\"grid_x\"],\n",
    "        project_config[\"pixel_y\"],\n",
    "        project_config[\"pixel_x\"],\n",
    "    )\n",
    "    cluster = extend_grid(cluster, image_size)\n",
    "    tracking_dir = os.path.join(project_config[\"res_dir\"], \"tracking\")\n",
    "    infer_sequence(\n",
    "        model,\n",
    "        dataset_dict,\n",
    "        model_dict,\n",
    "        project_config,\n",
    "        cluster,\n",
    "        min_mask_size=train_config[\"train_dict\"][\"min_mask_size\"] * 0.5,\n",
    "    )\n",
    "    foi_correction(tracking_dir, data_set)\n",
    "    fill_empty_frames(tracking_dir)\n",
    "    lineage = pd.read_csv(\n",
    "        os.path.join(tracking_dir, \"res_track.txt\"), sep=\" \", header=None\n",
    "    )\n",
    "    max_id = lineage[0].index.max()\n",
    "    if max_id >= 2 ** 16 - 1:\n",
    "        raise AssertionError(\n",
    "            \"Max Track id > 2**16 - uint16 transformation needed for ctc\"\n",
    "            \" measure will lead to buffer overflow!\"\n",
    "        )\n",
    "    rename_to_ctc_format(tracking_dir, ctc_res_path)\n",
    "    shutil.rmtree(temp_res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e18d2-7ed9-4c52-9864-12a7c076fdd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data_id in data_ids:\n",
    "    data_path = os.path.join(data_dir, data_id)\n",
    "    temp_results_path = os.path.join(data_dir, f'{data_id}_RES')\n",
    "    final_results_path = os.path.join(data_dir, f'{data_id}_RES')\n",
    "    if os.path.exists(final_results_path):\n",
    "        print(f'Skipping {data_id}, already complete')\n",
    "        continue\n",
    "\n",
    "    inference(data_path, model_path, config_file, batch_size=batch_size)\n",
    "\n",
    "    # Move results into the results subdirectory\n",
    "    shutil.move(temp_results_path, final_results_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
